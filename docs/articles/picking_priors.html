<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="fido">
<title>Picking Priors • fido</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Picking Priors">
<meta property="og:description" content="fido">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light" data-bs-theme="light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">fido</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">1.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/introduction-to-fido.html">Introduction to fido::Pibble</a>
    <a class="dropdown-item" href="../articles/non-linear-models.html">Non-linear models with fido::basset</a>
    <a class="dropdown-item" href="../articles/orthus.html">Joint Modeling  (e.g., Multiomics) with fido::Orthus</a>
    <div class="dropdown-divider"></div>
    <a class="dropdown-item" href="../articles/index.html">More articles...</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="navbar-nav">
<li><form class="form-inline" role="search">
<input type="search" class="form-control" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/jsilve24/fido/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Picking Priors</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/jsilve24/fido/blob/HEAD/vignettes/picking_priors.Rmd" class="external-link"><code>vignettes/picking_priors.Rmd</code></a></small>
      <div class="d-none name"><code>picking_priors.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>Picking priors is both an important and difficult part of Bayesian
statistics. This vignette is not intended to be an introduction to
Bayesian statistics, here I assume readers are already know what a
prior/posterior is. Just to review, a prior is a probability
distribution representing an analysts belief in model parameters prior
to seeing the data. The posterior is the (in some sense optimal)
probability distribution representing what you should belief having seen
the data (given your prior beliefs).</p>
<p>Since priors represent an analysts belief prior to seeing the data,
it makes sense that priors will often be specific for a given study. For
example, we don’t necessarily believe the parameters learned for an
RNA-seq data analysis will be the same as for someone studying microbial
communities or political gerrymandering. What’s more, we probably have
different prior beliefs depending on what microbial community we are
studying or how a study is set up.</p>
<p>There are (at least) two important reasons to think carefully about
priors. First, the meaning of the posterior is conditioned on your prior
accurately reflecting your beliefs. The posterior represents an optimal
belief given data and <em>given prior beliefs</em>. If a specified prior
does not reflect your beliefs well then the prior won’t have the right
meaning. Of course all priors are imperfect but we do the best we can.
Second, on a practical note, some really weird priors can lead to
numerical issues during optimization and uncertainty quantification in
<em>fido</em>. This later problem can appear as failure to reach the MAP
estimate or an error when trying to invert the Hessian.</p>
<p>Overall, a prior is a single function (a probability distribution)
specified jointly on parameters of interest. Still, it can be confusing
to think about the prior in the joint form. Here I will instead try to
simplify this and break the prior down into distinct components. While
there are numerous models in <em>fido</em>, here I will focus on the
prior for the <em>pibble</em> model as it is, in my opinion, the heart
of <em>fido</em>.</p>
<p>Just to review, the pibble model is given by: <span class="math display">\[
\begin{align}
Y_j &amp; \sim \text{Multinomial}\left(\pi_j \right)  \\
\pi_j &amp; = \phi^{-1}(\eta_j) \\
\eta_j &amp;\sim N(\Lambda X_j, \Sigma) \\
\Lambda &amp;\sim  N(\Theta, \Sigma, \Gamma) \\
\Sigma &amp;\sim W^{-1}(\Xi, \upsilon).
\end{align}
\]</span></p>
<p>We consider the first two lines to be part of the likelihood and the
bottom three lines to be part of the prior. Therefore we have the
following three components of the prior:</p>
<ul>
<li>The prior for <span class="math inline">\(\Sigma\)</span>: <span class="math inline">\(\Sigma \sim W^{-1}(\Xi, \upsilon)\)</span>
</li>
<li>The prior for <span class="math inline">\(\Lambda\)</span>: <span class="math inline">\(\Lambda \sim N(\Theta, \Sigma,
\Gamma)\)</span>
</li>
<li>The prior for <span class="math inline">\(\eta_j\)</span>: <span class="math inline">\(\eta_j \sim N(\Lambda X_j, \Sigma)\)</span>
</li>
</ul>
</div>
<div class="section level2">
<h2 id="background-on-the-matrix-normal">Background on the Matrix Normal<a class="anchor" aria-label="anchor" href="#background-on-the-matrix-normal"></a>
</h2>
<p>There are three things I should explain before going forward. The vec
operation, the Kronecker product, and the matrix normal. The first two
are needed to understand the matrix-normal.</p>
<div class="section level3">
<h3 id="the-vec-operation">The Vec Operation<a class="anchor" aria-label="anchor" href="#the-vec-operation"></a>
</h3>
<p>The vec operation is just a special way of saying column stacking. If
we have a<br>
matrix <span class="math display">\[X = \begin{bmatrix} a &amp; b \\ c
&amp; d \end{bmatrix}\]</span> then <span class="math display">\[vec(X)
= \begin{bmatrix} a\\ c \\ b\\d\end{bmatrix}.\]</span> It’s that
simple.</p>
</div>
<div class="section level3">
<h3 id="kronker-products">Kronker Products<a class="anchor" aria-label="anchor" href="#kronker-products"></a>
</h3>
<p>It turns out there are many different definitions for how to multiply
two matrices together. There is standard matrix multiplication, there is
element-wise multiplication, there is also something called the
Kronecker product. Given two matrices <span class="math inline">\(X =
\begin{bmatrix} x_{11} &amp; x_{12} \\ x_{21} &amp; x_{22}
\end{bmatrix}\)</span> and <span class="math inline">\(Y =
\begin{bmatrix} y_{11} &amp; y_{12} \\ y_{21} &amp; y_{22}
\end{bmatrix}\)</span>, we define the Kronecker product of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as <span class="math display">\[
X \otimes Y = \begin{bmatrix}x_{11}Y &amp; x_{12}Y \\ x_{21}Y &amp;
x_{22}Y \end{bmatrix} =
\begin{bmatrix}
x_{11}y_{11} &amp; x_{11}y_{12} &amp; x_{12}y_{11} &amp; x_{12}y_{12} \\
x_{11}y_{21} &amp; x_{11}y_{22} &amp; x_{12}y_{21} &amp; x_{12}y_{22}
\\                
x_{21}y_{11} &amp; x_{21}y_{12} &amp; x_{22}y_{11} &amp; x_{22}y_{12} \\
x_{21}y_{21} &amp; x_{21}y_{22} &amp; x_{22}y_{21} &amp; x_{22}y_{22}
\\  
\end{bmatrix}.
\]</span> Notice how we are essentially making a larger matrix by
patterning Y over X?</p>
</div>
<div class="section level3">
<h3 id="the-matrix-normal">The Matrix Normal<a class="anchor" aria-label="anchor" href="#the-matrix-normal"></a>
</h3>
<p>Before going forward you may be wondering why the normal in the prior
for <span class="math inline">\(\Lambda\)</span> has three parameters
(<span class="math inline">\(\Theta\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(\Gamma\)</span>) rather than two. This means that
our prior for <span class="math inline">\(\Lambda\)</span> is a
<em>matrix normal</em> rather than a <em>multivariate normal</em>. The
matrix normal is a generalization of the multivariate normal for random
matrices (not just random vectors). Below is a simplified description of
the matrix normal.</p>
<p>With the multivariate normal you have a mean vector and a covariance
matrix describing the spread of the distribution about the mean. With
the matrix normal you have a mean matrix, and two covariance matrices
describing the spread of the distribution about the mean. The first
covariance matrix (<span class="math inline">\(\Sigma\)</span>)
describes the covariance between the rows of <span class="math inline">\(\Lambda\)</span> while the second covariance
matrix (<span class="math inline">\(\Gamma\)</span>). describes the
covariance between the columns of <span class="math inline">\(\Lambda\)</span>.</p>
<p>The relationship between the multivariate normal and the matrix
normal is as follows. <span class="math display">\[\Lambda \sim
N(\Theta, \Sigma, \Gamma) \leftrightarrow vec(\Lambda) \sim
N(vec(\Theta), \Gamma \otimes \Sigma)\]</span> where <span class="math inline">\(\otimes\)</span> represents the Kronecker product
and <span class="math inline">\(vec\)</span> represents the
vectorization operation (i.e., column stacking of a matrix to produce a
very long vector).</p>
<p>So we can now ask, what is the distribution of a single element of
<span class="math inline">\(\Lambda\)</span>? The answer is simply <span class="math display">\[\Lambda_{ij} \sim N(\Theta_{ij},
\Sigma_{ii}\Gamma_{jj}).\]</span> Similarly, we can ask about the
distribution of a single column of <span class="math inline">\(\Lambda\)</span>: <span class="math display">\[\Lambda_{\cdot j} \sim N(\Theta_{\cdot j},
\Gamma_{jj} \Sigma).\]</span> Make sense? If not take a look at <a href="https://en.wikipedia.org/wiki/Matrix_normal_distribution" class="external-link">wikipedia
for a more complete treatment of the matrix-normal</a>.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-prior-for-sigma">The prior for <span class="math inline">\(\Sigma\)</span><a class="anchor" aria-label="anchor" href="#the-prior-for-sigma"></a>
</h2>
<p><span class="math inline">\(\Sigma\)</span> describes the covariance
between log-ratios. So if <span class="math inline">\(\phi^{-1}\)</span>
is the inverse of the <span class="math inline">\(ALR_D\)</span>
transform then <span class="math inline">\(\Sigma\)</span> describes the
covariance between <span class="math inline">\(ALR_D\)</span>
coordinates. Also note, this section is going to be the hardest one, the
other priors components will be faster to describe and probably easier
to understand.</p>
<div class="section level3">
<h3 id="background-on-the-prior">Background on the Prior<a class="anchor" aria-label="anchor" href="#background-on-the-prior"></a>
</h3>
<p>The prior for <span class="math inline">\(\Sigma\)</span> is an <a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution" class="external-link">Inverse
Wishart</a> written <span class="math display">\[\Sigma \sim W^{-1}(\Xi,
\upsilon)\]</span> where <span class="math inline">\(\Xi\)</span> is
called the scale matrix (and must be a valid covariance matrix itself),
and <span class="math inline">\(\upsilon\)</span> is called the degrees
of freedom parameter. If <span class="math inline">\(\Sigma\)</span> is
a <span class="math inline">\((D-1)x(D-1)\)</span> matrix, then there is
a constraint on <span class="math inline">\(\upsilon\)</span> such that
<span class="math inline">\(\upsilon \geq D-1\)</span>.</p>
<p>The inverse Wishart has a mildly complex form for its moments (e.g.,
mean and variance). Its mean is given by <span class="math display">\[E[\Sigma] = \frac{\Xi}{\upsilon-D-2} \quad
\text{for } \upsilon &gt; D.\]</span> Its variance is somewhat
complicated (<a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution#Moments" class="external-link">Wikipedia
gives the relationships</a>) but for most purposes you can think of
<span class="math inline">\(\upsilon\)</span> as setting the variance,
larger <span class="math inline">\(\upsilon\)</span> means less
uncertainty (lower variance) about the mean, smaller <span class="math inline">\(\upsilon\)</span> means more uncertainty (higher
variance) about the mean.</p>
</div>
<div class="section level3">
<h3 id="choosing-upsilon-and-xi-">Choosing <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span>.<a class="anchor" aria-label="anchor" href="#choosing-upsilon-and-xi-"></a>
</h3>
<p>Reading the above may seem intimidating: that’s the form of the mean…
so what? What’s-more how should I think about covariance between
log-ratios? Here’s how I think about it. I think about it in-terms of
putting a prior on the true abundances in log-space and then
transforming that into a prior on log-ratios. Before I can really
explain that I need to explain a bit more background.</p>
<p><strong>Compositional Data Analysis in a Nutshell</strong> It turns
out that those transforms <span class="math inline">\(\phi\)</span> are
all examples of log-ratio transforms studied in a field called
compositional data analysis. Briefly, all of those transforms can be
written in a form: <span class="math inline">\(\eta = \Psi \log
\pi\)</span>. So log-ratios (<span class="math inline">\(\eta\)</span>)
are just a linear transform of log-transformed relative-abundances. It
turns out that because of special properties of <span class="math inline">\(\Psi\)</span>, the following also holds: <span class="math inline">\(\eta = \Psi \log w\)</span> where <span class="math inline">\(w\)</span> are the absolute (not relative)
abundances. So we can say that log-ratios are also just a linear
transform of log-transformed absolute-abundances.</p>
<p><strong>Linear Transformations of Covariance Matricies</strong>
Recall that if <span class="math inline">\(x \sim N(\mu,
\Sigma)\)</span> (for multivariate <span class="math inline">\(x\)</span>) then for a matrix <span class="math inline">\(\Psi\)</span> we have <span class="math inline">\(\Psi x \sim N(\Psi \mu, \Psi \Sigma
\Psi^T)\)</span>. This is to say that you should think of linear
transformations of covariance matrices as being applied by pre <em>and
post</em> multiplying by the transformation matrix <span class="math inline">\(\Psi\)</span>.</p>
<p><strong>Linear transformation of the Inverse Wishart</strong> It
turns out that if we have <span class="math inline">\(\Omega \sim
W^{-1}(\gamma, S)\)</span> for a <span class="math inline">\(D\times
D\)</span> covariance matrix <span class="math inline">\(\Omega\)</span>
then for <span class="math inline">\(M\times D\)</span> matrix <span class="math inline">\(\Psi\)</span> we have <span class="math inline">\(\Psi \Omega \Psi^T \sim W^{-1}(\upsilon, \Psi S
\Psi^T)\)</span>.</p>
<p><strong>Putting It All Together</strong> A central question: what is
a reasonable prior for log-ratios? We are not used to working with
log-ratios so this is difficult. A potentially simpler problem is to
place a prior on the log-absolute-abundances (<span class="math inline">\(\Omega\)</span>) of whatever we are measuring,
<em>e.g.</em>, placing a prior on the covariance between
log-absolute-abundances of bacteria (<span class="math inline">\(\Omega
\sim W^{-1}(\gamma, S)\)</span>.</p>
<p><em>An example:</em> Lets say that for a given microbiome dataset, I
have weak prior belief that, on average, all the taxa are independent
with variance 1. I want to come up with values <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(S\)</span> for my prior on <span class="math inline">\(\Omega\)</span> that reflect this. Lets start by
specifying the mean for <span class="math inline">\(\Omega\)</span>.
<span class="math display">\[E[\Omega] =I_D.\]</span> Next we say we
have little certainty about this mean (want high variance) so we set
<span class="math inline">\(\gamma\)</span> to be close to the lower
bound of <span class="math inline">\(D\)</span> (I often like <span class="math inline">\(\gamma=D+3\)</span>). Now we have <span class="math inline">\(\gamma\)</span> we need to calculate <span class="math inline">\(S\)</span> which we do by solving for <span class="math inline">\(S\)</span> in the equation for the Inverse-Wishart
mean<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Note its &lt;span class="math inline"&gt;\(D-1\)&lt;/span&gt; here
because &lt;span class="math inline"&gt;\(\Omega\)&lt;/span&gt; is &lt;span class="math inline"&gt;\(D\times D\)&lt;/span&gt; rather than &lt;span class="math inline"&gt;\(D-1 \times D-1\)&lt;/span&gt;&lt;/p&gt;'><sup>1</sup></a>:
<span class="math display">\[S = E[\Omega](\gamma -D-1).\]</span> There
you go that’s a prior on the log-absolute-abundances. Next we need to
transform this into a prior on log-ratios. Well the above allows us to
do this by simplifying taking the contrast matrix <span class="math inline">\(\Psi\)</span> from the log-ratio transform we want
and transforming our prior for <span class="math inline">\(\Omega\)</span> as <span class="math inline">\(\Sigma \sim W^{-1}(\gamma, \Psi S
\Psi^T)\)</span>. That’s it there the prior on log-ratios built form a
prior on log-absolute-abundances.</p>
<p><em>A Note on Phylogenetic priors:</em> As in phylogenetic linear
models, you can make <span class="math inline">\(S\)</span> (as defined
above) a covariance derived from the phylogenetic differences between
taxa. This allows you to fit phylogenetic linear models in
<em>fido</em>.</p>
<p><strong>Making It Even Simpler</strong> Say that you have a prior
<span class="math inline">\(\Omega \sim W^{-1}(\gamma, S)\)</span> for
covariance between log-absolute-abundances (created as in our example
above). You want to transform this into a prior <span class="math inline">\(\Sigma \sim W^{-1}(\upsilon, \Xi)\)</span>. You do
this by simply taking <span class="math inline">\(\upsilon=\gamma\)</span>. To calculate <span class="math inline">\(\Xi\)</span>, rather than worrying about <span class="math inline">\(\Psi\)</span>, functions in the <a href="https://jsilve24.github.io/driver/" class="external-link"><em>driver</em> package I
wrote</a> will do this for you, here are recipes:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># To put prior on ALR_j coordinates for some j in (1,...,D-1)</span></span>
<span><span class="va">Xi</span> <span class="op">&lt;-</span> <span class="fu">clrvar2alrvar</span><span class="op">(</span><span class="va">S</span>, <span class="va">j</span><span class="op">)</span></span>
<span><span class="co"># To put prior in a particular ILR coordinate defined by contrast matrix V</span></span>
<span><span class="va">Xi</span> <span class="op">&lt;-</span> <span class="fu">clrvar2ilrvar</span><span class="op">(</span><span class="va">S</span>, <span class="va">V</span><span class="op">)</span></span>
<span><span class="co"># To put prior in CLR coordinates (this one needs two transforms)</span></span>
<span><span class="va">foo</span> <span class="op">&lt;-</span> <span class="fu">clrvar2alrvar</span><span class="op">(</span><span class="va">S</span>, <span class="va">D</span><span class="op">)</span></span>
<span><span class="va">Xi</span> <span class="op">&lt;-</span> <span class="fu">alrvar2clrvar</span><span class="op">(</span><span class="va">foo</span>, <span class="va">D</span><span class="op">)</span></span></code></pre></div>
<p>Hopefully that is simple enough to be useful for folks.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-prior-for-lambda">The prior for <span class="math inline">\(\Lambda\)</span><a class="anchor" aria-label="anchor" href="#the-prior-for-lambda"></a>
</h2>
<p><span class="math inline">\(\Lambda\)</span> are the regression
parameters in the linear model. The prior for <span class="math inline">\(\Lambda\)</span> is just a matrix-normal which we
described above: <span class="math display">\[\Lambda \sim N(\Theta,
\Sigma, \Gamma).\]</span> Here <span class="math inline">\(\Theta\)</span> is the mean matrix of <span class="math inline">\(\Lambda\)</span>, <span class="math inline">\(\Sigma\)</span> is actually random (i.e., you
don’t have to specify it, its specified by the prior on <span class="math inline">\(\Sigma\)</span> we discussed already), and <span class="math inline">\(\Gamma\)</span> is a <span class="math inline">\(QxQ\)</span><a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Where &lt;span class="math inline"&gt;\(Q\)&lt;/span&gt; is the
number of regression covaraites&lt;/p&gt;'><sup>2</sup></a> covariance matrix describing the covariance
between the columns of <span class="math inline">\(\Lambda\)</span>
(<em>i.e.</em>, between the effect of the different covariates). So we
really need to just discuss specifying <span class="math inline">\(\Theta\)</span> and specifying <span class="math inline">\(\Gamma\)</span>.</p>
<div class="section level3">
<h3 id="choosing-theta">Choosing <span class="math inline">\(\Theta\)</span><a class="anchor" aria-label="anchor" href="#choosing-theta"></a>
</h3>
<p>This is really easy, in most situations this will simply be a matrix
of zeros. This implies that you expect that on average, the covariates
of interest are not associated with composition.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;This is the same assumption as used in Ridge
Regression&lt;/p&gt;"><sup>3</sup></a>. This helps prevent
you from inferrign an effect if there isn’t one.</p>
<p>Outside of this simple case lets say you actually have prior
knowledge about the effects of the covariates. Perhaps you have some
knowledge about the mean effect of covariates on log-absolute-abundances
which you describe in a <span class="math inline">\(D\times Q\)</span>
matrix <span class="math inline">\(A\)</span>. Well you can just
transform that prior into the log-ratio coordinates you want as
follows:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Transform from log-absolute-abundance effects to effects on absolute-abundances</span></span>
<span><span class="va">foo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">A</span><span class="op">)</span></span>
<span><span class="co"># To put prior on ALR_j coordinates for some j in (1,...,D-1)</span></span>
<span><span class="va">Theta</span> <span class="op">&lt;-</span> <span class="fu">driver</span><span class="fu">::</span><span class="fu"><a href="https://jsilve24.github.io/driver/reference/array_lr_transforms.html" class="external-link">alr_array</a></span><span class="op">(</span><span class="va">foo</span>, <span class="va">j</span>, parts<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co"># To put prior in a particular ILR coordinate defined by contrast matrix V</span></span>
<span><span class="va">Theta</span> <span class="op">&lt;-</span> <span class="fu">driver</span><span class="fu">::</span><span class="fu"><a href="https://jsilve24.github.io/driver/reference/array_lr_transforms.html" class="external-link">ilr_array</a></span><span class="op">(</span><span class="va">foo</span>, <span class="va">V</span>, parts<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co"># To put prior in CLR coordinates</span></span>
<span><span class="va">Theta</span> <span class="op">&lt;-</span> <span class="fu">driver</span><span class="fu">::</span><span class="fu"><a href="https://jsilve24.github.io/driver/reference/array_lr_transforms.html" class="external-link">clr_array</a></span><span class="op">(</span><span class="va">foo</span>, parts<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="choosing-gamma">Choosing <span class="math inline">\(\Gamma\)</span><a class="anchor" aria-label="anchor" href="#choosing-gamma"></a>
</h3>
<p>Alright, here we get a break as <span class="math inline">\(\Gamma\)</span> doesn’t care what log-ratio
coordinates your in. It’s just a <span class="math inline">\(Q\times
Q\)</span> covariance matrix describing the covariation between the
effects of the <span class="math inline">\(Q\)</span> covariates.</p>
<p>For example, Lets say your data is a microbiome survey of a disease
with a number of healthy controls. Your goal is to figure out what is
different between the composition of these two groups. Your model may
have two covariates, an intercept and a binary variable (1 if sample is
from disease, 0 if from healthy). We probably want to set a prior that
allows the intercept to be moderately large but we likely believe that
the differences between disease and health are small (so we want the
effect of the binary covariate to be modest). We could specify: <span class="math display">\[\Gamma = \alpha\begin{bmatrix} 1 &amp; 0 \\
0&amp; .2 \end{bmatrix}\]</span> for a scalar <span class="math inline">\(\alpha\)</span> which I will discuss in depth
below. Note here the off diagonals being zero also specifies that we
don’t think there is any covariation between the intercept and the
effect of the disease state (probably a pretty good assumption in this
example).</p>
<p>The choice of alpha can be important. I will describe it later in the
section on how the choice of <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span> interact with the choice of <span class="math inline">\(\Gamma\)</span>. First I need to briefly describe
the prior on <span class="math inline">\(\eta\)</span>.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-prior-for-eta">The Prior for <span class="math inline">\(\eta\)</span><a class="anchor" aria-label="anchor" href="#the-prior-for-eta"></a>
</h2>
<p><span class="math inline">\(\eta\)</span> are log-ratios from the
regression relationship obscured by noise. <span class="math display">\[\eta_j \sim N(\Lambda X_j, \Sigma).\]</span>
Notice <span class="math inline">\(\Sigma\)</span> shows up again like
it did in the prior for <span class="math inline">\(\Lambda\)</span>.
Actually, there are no more parameters we need to specify, the prior for
<span class="math inline">\(\eta\)</span> is completely induced based on
our priors for <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\Sigma\)</span>. The reason I discuss it here is
that I want readers to recognize that the variation of <span class="math inline">\(\eta\)</span> about the regression relationship is
specified by <span class="math inline">\(\Sigma\)</span>. That means
that if <span class="math inline">\(\Sigma\)</span> is large there is
more noise, small there is less noise. This should also be taken into
account when specifying <span class="math inline">\(\upsilon\)</span>
and <span class="math inline">\(\Xi\)</span>. The next section will
further expand on this idea.</p>
</div>
<div class="section level2">
<h2 id="how-the-choice-of-upsilon-and-xi-interacts-with-the-choice-of-gamma">How the Choice of <span class="math inline">\(\upsilon\)</span> and
<span class="math inline">\(\Xi\)</span> Interacts With the Choice of
<span class="math inline">\(\Gamma\)</span><a class="anchor" aria-label="anchor" href="#how-the-choice-of-upsilon-and-xi-interacts-with-the-choice-of-gamma"></a>
</h2>
<p>The point of this subsection is the following, the choice of <span class="math inline">\(\Gamma\)</span>, <span class="math inline">\(\Xi\)</span>, and <span class="math inline">\(\upsilon\)</span> in some senses place a prior on
the signal-to-noise ratio in the data. In short: The larger <span class="math inline">\(\Gamma\)</span> is relative to <span class="math inline">\(\Sigma\)</span> (specified by <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span>) the more signal, the smaller <span class="math inline">\(\Gamma\)</span> is realtive to <span class="math inline">\(\Sigma\)</span> the more noise. I will describe
this below.</p>
<p>Notice that we could alternatively write the prior for <span class="math inline">\(\eta\)</span> as <span class="math display">\[\eta
\sim N(\Lambda X, \Sigma, I)\]</span> using the matrix normal in
parallel to our prior for <span class="math inline">\(\Lambda\)</span>
<span class="math display">\[\Lambda \sim N(\Theta, \Sigma,
\Gamma).\]</span> We can write the <em>vec</em> form of these
relationships as <span class="math display">\[
\begin{align}
vec(\eta) &amp;\sim N(vec(\Lambda X), I\otimes\Sigma) \\
vec(\Lambda) &amp;\sim N(vec(\Theta), \Gamma \otimes \Sigma).
\end{align}
\]</span> If we write <span class="math inline">\(\Gamma\)</span> as the
multiplication of a scalar and scaled matrix (a matrix scaled so that
the sum of the diagonals equals 1) <span class="math inline">\(\Gamma=\alpha \bar{\Gamma}\)</span> as we did when
describing the choice of <span class="math inline">\(\Gamma\)</span>
above, then the above equations turn into: <span class="math display">\[
\begin{align}
vec(\eta) &amp;\sim N(vec(\Lambda X), 1(I\otimes\Sigma)) \\
vec(\Lambda) &amp;\sim N(vec(\Theta), \alpha(\bar{\Gamma}\otimes
\Sigma)).
\end{align}
\]</span> and we can see that the magnitude of <span class="math inline">\(\Lambda\)</span> is a factor of <span class="math inline">\(\alpha\)</span> times the noise level. If <span class="math inline">\(\alpha&lt;1\)</span> we have that the the
magnitude of <span class="math inline">\(\Lambda\)</span> is smaller
than the magnitude of the noise. If <span class="math inline">\(\alpha
&gt; 1\)</span> we have that the magnitude of <span class="math inline">\(\Lambda\)</span> is greater than the magnitude of
the noise.</p>
<p>The actual “signal” is the product <span class="math inline">\(\Lambda X\)</span> (so it depends on the scale of
<span class="math inline">\(X\)</span>) as well but hopefully the point
is clear: <strong>The magnitude of <span class="math inline">\(\Sigma\)</span> (which is specified by <span class="math inline">\(\upsilon\)</span> and <span class="math inline">\(\Xi\)</span>) in comparision to the magnitude of
<span class="math inline">\(\Gamma\)</span> sets the signal-to-noise
ratio in our prior.</strong></p>
</div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://www.justin-silverman.com/" class="external-link">Justin Silverman</a>, Michelle Nixon.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
