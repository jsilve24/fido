<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Picking Priors • fido</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Picking Priors">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">fido</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">1.1.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/introduction-to-fido.html">Introduction to fido::Pibble</a></li>
    <li><a class="dropdown-item" href="../articles/non-linear-models.html">Non-linear models with fido::basset</a></li>
    <li><a class="dropdown-item" href="../articles/orthus.html">Joint Modeling  (e.g., Multiomics) with fido::Orthus</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../articles/index.html">More articles...</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/jsilve24/fido/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Picking Priors</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/jsilve24/fido/blob/master/vignettes/picking_priors.Rmd" class="external-link"><code>vignettes/picking_priors.Rmd</code></a></small>
      <div class="d-none name"><code>picking_priors.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>Picking priors is both an important and difficult part of Bayesian
statistics. This vignette is not intended to be an introduction to
Bayesian statistics, here I assume readers are already know what a
prior/posterior is. Just to review, a prior is a probability
distribution representing an analysts belief in model parameters prior
to seeing the data. The posterior is the (in some sense optimal)
probability distribution representing what you should belief having seen
the data (given your prior beliefs).</p>
<p>Since priors represent an analysts belief prior to seeing the data,
it makes sense that priors will often be specific for a given study. For
example, we don’t necessarily believe the parameters learned for an
RNA-seq data analysis will be the same as for someone studying microbial
communities or political gerrymandering. What’s more, we probably have
different prior beliefs depending on what microbial community we are
studying or how a study is set up.</p>
<p>There are (at least) two important reasons to think carefully about
priors. First, the meaning of the posterior is conditioned on your prior
accurately reflecting your beliefs. The posterior represents an optimal
belief given data and <em>given prior beliefs</em>. If a specified prior
does not reflect your beliefs well then the prior won’t have the right
meaning. Of course all priors are imperfect but we do the best we can.
Second, on a practical note, some really weird priors can lead to
numerical issues during optimization and uncertainty quantification in
<em>fido</em>. This later problem can appear as failure to reach the MAP
estimate or an error when trying to invert the Hessian.</p>
<p>Overall, a prior is a single function (a probability distribution)
specified jointly on parameters of interest. Still, it can be confusing
to think about the prior in the joint form. Here I will instead try to
simplify this and break the prior down into distinct components. While
there are numerous models in <em>fido</em>, here I will focus on the
prior for the <em>pibble</em> model as it is, in my opinion, the heart
of <em>fido</em>.</p>
<p>Just to review, the pibble model is given by:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>Y</mi><mi>j</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mtext mathvariant="normal">Multinomial</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>π</mi><mi>j</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>ϕ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>η</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>η</mi><mi>j</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mi>X</mi><mi>j</mi></msub><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>Λ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Θ</mi><mo>,</mo><mi>Σ</mi><mo>,</mo><mi>Γ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>Σ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ξ</mi><mo>,</mo><mi>υ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align}
Y_j &amp; \sim \text{Multinomial}\left(\pi_j \right)  \\
\pi_j &amp; = \phi^{-1}(\eta_j) \\
\eta_j &amp;\sim N(\Lambda X_j, \Sigma) \\
\Lambda &amp;\sim  N(\Theta, \Sigma, \Gamma) \\
\Sigma &amp;\sim W^{-1}(\Xi, \upsilon). 
\end{align}
</annotation></semantics></math></p>
<p>We consider the first two lines to be part of the likelihood and the
bottom three lines to be part of the prior. Therefore we have the
following three components of the prior:</p>
<ul>
<li>The prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Σ</mi><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ξ</mi><mo>,</mo><mi>υ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Sigma \sim W^{-1}(\Xi, \upsilon)</annotation></semantics></math>
</li>
<li>The prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Λ</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Θ</mi><mo>,</mo><mi>Σ</mi><mo>,</mo><mi>Γ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Lambda \sim  N(\Theta, \Sigma, \Gamma)</annotation></semantics></math>
</li>
<li>The prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>η</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\eta_j</annotation></semantics></math>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>η</mi><mi>j</mi></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mi>X</mi><mi>j</mi></msub><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\eta_j \sim N(\Lambda X_j, \Sigma)</annotation></semantics></math>
</li>
</ul>
</div>
<div class="section level2">
<h2 id="background-on-the-matrix-normal">Background on the Matrix Normal<a class="anchor" aria-label="anchor" href="#background-on-the-matrix-normal"></a>
</h2>
<p>There are three things I should explain before going forward. The vec
operation, the Kronecker product, and the matrix normal. The first two
are needed to understand the matrix-normal.</p>
<div class="section level3">
<h3 id="the-vec-operation">The Vec Operation<a class="anchor" aria-label="anchor" href="#the-vec-operation"></a>
</h3>
<p>The vec operation is just a special way of saying column stacking. If
we have a<br>
matrix
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>b</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">X = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}</annotation></semantics></math>
then
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>c</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>b</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">vec(X) = \begin{bmatrix} a\\ c \\ b\\d\end{bmatrix}.</annotation></semantics></math>
It’s that simple.</p>
</div>
<div class="section level3">
<h3 id="kronker-products">Kronker Products<a class="anchor" aria-label="anchor" href="#kronker-products"></a>
</h3>
<p>It turns out there are many different definitions for how to multiply
two matrices together. There is standard matrix multiplication, there is
element-wise multiplication, there is also something called the
Kronecker product. Given two matrices
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>12</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>22</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">X = \begin{bmatrix} x_{11} &amp; x_{12} \\ x_{21} &amp; x_{22} \end{bmatrix}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>y</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>y</mi><mn>12</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>y</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>y</mi><mn>22</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Y = \begin{bmatrix} y_{11} &amp; y_{12} \\ y_{21} &amp; y_{22} \end{bmatrix}</annotation></semantics></math>,
we define the Kronecker product of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>⊗</mo><mi>Y</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>11</mn></msub><mi>Y</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>12</mn></msub><mi>Y</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>21</mn></msub><mi>Y</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>22</mn></msub><mi>Y</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>11</mn></msub><msub><mi>y</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>11</mn></msub><msub><mi>y</mi><mn>12</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>12</mn></msub><msub><mi>y</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>12</mn></msub><msub><mi>y</mi><mn>12</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>11</mn></msub><msub><mi>y</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>11</mn></msub><msub><mi>y</mi><mn>22</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>12</mn></msub><msub><mi>y</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>12</mn></msub><msub><mi>y</mi><mn>22</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>21</mn></msub><msub><mi>y</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>21</mn></msub><msub><mi>y</mi><mn>12</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>22</mn></msub><msub><mi>y</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>22</mn></msub><msub><mi>y</mi><mn>12</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>21</mn></msub><msub><mi>y</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>21</mn></msub><msub><mi>y</mi><mn>22</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>22</mn></msub><msub><mi>y</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>22</mn></msub><msub><mi>y</mi><mn>22</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
X \otimes Y = \begin{bmatrix}x_{11}Y &amp; x_{12}Y \\ x_{21}Y &amp; x_{22}Y \end{bmatrix} = 
\begin{bmatrix} 
x_{11}y_{11} &amp; x_{11}y_{12} &amp; x_{12}y_{11} &amp; x_{12}y_{12} \\
x_{11}y_{21} &amp; x_{11}y_{22} &amp; x_{12}y_{21} &amp; x_{12}y_{22} \\                
x_{21}y_{11} &amp; x_{21}y_{12} &amp; x_{22}y_{11} &amp; x_{22}y_{12} \\
x_{21}y_{21} &amp; x_{21}y_{22} &amp; x_{22}y_{21} &amp; x_{22}y_{22} \\  
\end{bmatrix}.
</annotation></semantics></math> Notice how we are essentially making a
larger matrix by patterning Y over X?</p>
</div>
<div class="section level3">
<h3 id="the-matrix-normal">The Matrix Normal<a class="anchor" aria-label="anchor" href="#the-matrix-normal"></a>
</h3>
<p>Before going forward you may be wondering why the normal in the prior
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
has three parameters
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Θ</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>)
rather than two. This means that our prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
is a <em>matrix normal</em> rather than a <em>multivariate normal</em>.
The matrix normal is a generalization of the multivariate normal for
random matrices (not just random vectors). Below is a simplified
description of the matrix normal.</p>
<p>With the multivariate normal you have a mean vector and a covariance
matrix describing the spread of the distribution about the mean. With
the matrix normal you have a mean matrix, and two covariance matrices
describing the spread of the distribution about the mean. The first
covariance matrix
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>)
describes the covariance between the rows of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
while the second covariance matrix
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>).
describes the covariance between the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>.</p>
<p>The relationship between the multivariate normal and the matrix
normal is as follows.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Λ</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Θ</mi><mo>,</mo><mi>Σ</mi><mo>,</mo><mi>Γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>↔</mo><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>Γ</mi><mo>⊗</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Lambda \sim N(\Theta, \Sigma, \Gamma) \leftrightarrow vec(\Lambda) \sim N(vec(\Theta), \Gamma \otimes \Sigma)</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊗</mo><annotation encoding="application/x-tex">\otimes</annotation></semantics></math>
represents the Kronecker product and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mi>e</mi><mi>c</mi></mrow><annotation encoding="application/x-tex">vec</annotation></semantics></math>
represents the vectorization operation (i.e., column stacking of a
matrix to produce a very long vector).</p>
<p>So we can now ask, what is the distribution of a single element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>?
The answer is simply
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Λ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Θ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>Σ</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><msub><mi>Γ</mi><mrow><mi>j</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\Lambda_{ij} \sim N(\Theta_{ij}, \Sigma_{ii}\Gamma_{jj}).</annotation></semantics></math>
Similarly, we can ask about the distribution of a single column of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Λ</mi><mrow><mo>⋅</mo><mi>j</mi></mrow></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Θ</mi><mrow><mo>⋅</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>Γ</mi><mrow><mi>j</mi><mi>j</mi></mrow></msub><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\Lambda_{\cdot j} \sim N(\Theta_{\cdot j}, \Gamma_{jj} \Sigma).</annotation></semantics></math>
Make sense? If not take a look at <a href="https://en.wikipedia.org/wiki/Matrix_normal_distribution" class="external-link">wikipedia
for a more complete treatment of the matrix-normal</a>.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-prior-for-sigma">The prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#the-prior-for-sigma"></a>
</h2>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
describes the covariance between log-ratios. So if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ϕ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\phi^{-1}</annotation></semantics></math>
is the inverse of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>L</mi><msub><mi>R</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">ALR_D</annotation></semantics></math>
transform then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
describes the covariance between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>L</mi><msub><mi>R</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">ALR_D</annotation></semantics></math>
coordinates. Also note, this section is going to be the hardest one, the
other priors components will be faster to describe and probably easier
to understand.</p>
<div class="section level3">
<h3 id="background-on-the-prior">Background on the Prior<a class="anchor" aria-label="anchor" href="#background-on-the-prior"></a>
</h3>
<p>The prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
is an <a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution" class="external-link">Inverse
Wishart</a> written
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Σ</mi><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ξ</mi><mo>,</mo><mi>υ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Sigma \sim W^{-1}(\Xi, \upsilon)</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>
is called the scale matrix (and must be a valid covariance matrix
itself), and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
is called the degrees of freedom parameter. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
is a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(D-1)x(D-1)</annotation></semantics></math>
matrix, then there is a constraint on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>υ</mi><mo>≥</mo><mi>D</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\upsilon \geq D-1</annotation></semantics></math>.</p>
<p>The inverse Wishart has a mildly complex form for its moments (e.g.,
mean and variance). Its mean is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>Σ</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mfrac><mi>Ξ</mi><mrow><mi>υ</mi><mo>−</mo><mi>D</mi><mo>−</mo><mn>2</mn></mrow></mfrac><mspace width="1.0em"></mspace><mrow><mtext mathvariant="normal">for </mtext><mspace width="0.333em"></mspace></mrow><mi>υ</mi><mo>&gt;</mo><mi>D</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">E[\Sigma] = \frac{\Xi}{\upsilon-D-2} \quad \text{for } \upsilon &gt; D.</annotation></semantics></math>
Its variance is somewhat complicated (<a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution#Moments" class="external-link">Wikipedia
gives the relationships</a>) but for most purposes you can think of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
as setting the variance, larger
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
means less uncertainty (lower variance) about the mean, smaller
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
means more uncertainty (higher variance) about the mean.</p>
</div>
<div class="section level3">
<h3 id="choosing-upsilon-and-xi-">Choosing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>.<a class="anchor" aria-label="anchor" href="#choosing-upsilon-and-xi-"></a>
</h3>
<p>Reading the above may seem intimidating: that’s the form of the mean…
so what? What’s-more how should I think about covariance between
log-ratios? Here’s how I think about it. I think about it in-terms of
putting a prior on the true abundances in log-space and then
transforming that into a prior on log-ratios. Before I can really
explain that I need to explain a bit more background.</p>
<p><strong>Compositional Data Analysis in a Nutshell</strong> It turns
out that those transforms
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>
are all examples of log-ratio transforms studied in a field called
compositional data analysis. Briefly, all of those transforms can be
written in a form:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>=</mo><mi>Ψ</mi><mo>log</mo><mi>π</mi></mrow><annotation encoding="application/x-tex">\eta = \Psi \log \pi</annotation></semantics></math>.
So log-ratios
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>)
are just a linear transform of log-transformed relative-abundances. It
turns out that because of special properties of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ψ</mi><annotation encoding="application/x-tex">\Psi</annotation></semantics></math>,
the following also holds:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>=</mo><mi>Ψ</mi><mo>log</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">\eta = \Psi \log w</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
are the absolute (not relative) abundances. So we can say that
log-ratios are also just a linear transform of log-transformed
absolute-abundances.</p>
<p><strong>Linear Transformations of Covariance Matricies</strong>
Recall that if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x \sim N(\mu, \Sigma)</annotation></semantics></math>
(for multivariate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>)
then for a matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ψ</mi><annotation encoding="application/x-tex">\Psi</annotation></semantics></math>
we have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ψ</mi><mi>x</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Ψ</mi><mi>μ</mi><mo>,</mo><mi>Ψ</mi><mi>Σ</mi><msup><mi>Ψ</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Psi x \sim N(\Psi \mu, \Psi \Sigma \Psi^T)</annotation></semantics></math>.
This is to say that you should think of linear transformations of
covariance matrices as being applied by pre <em>and post</em>
multiplying by the transformation matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ψ</mi><annotation encoding="application/x-tex">\Psi</annotation></semantics></math>.</p>
<p><strong>Linear transformation of the Inverse Wishart</strong> It
turns out that if we have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>,</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega \sim W^{-1}(\gamma, S)</annotation></semantics></math>
for a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>×</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">D\times D</annotation></semantics></math>
covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>
then for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">M\times D</annotation></semantics></math>
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ψ</mi><annotation encoding="application/x-tex">\Psi</annotation></semantics></math>
we have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ψ</mi><mi>Ω</mi><msup><mi>Ψ</mi><mi>T</mi></msup><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>υ</mi><mo>,</mo><mi>Ψ</mi><mi>S</mi><msup><mi>Ψ</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Psi \Omega \Psi^T \sim W^{-1}(\upsilon, \Psi S \Psi^T)</annotation></semantics></math>.</p>
<p><strong>Putting It All Together</strong> A central question: what is
a reasonable prior for log-ratios? We are not used to working with
log-ratios so this is difficult. A potentially simpler problem is to
place a prior on the log-absolute-abundances
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>)
of whatever we are measuring, <em>e.g.</em>, placing a prior on the
covariance between log-absolute-abundances of bacteria
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>,</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega \sim W^{-1}(\gamma, S)</annotation></semantics></math>.</p>
<p><em>An example:</em> Lets say that for a given microbiome dataset, I
have weak prior belief that, on average, all the taxa are independent
with variance 1. I want to come up with values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
for my prior on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>
that reflect this. Lets start by specifying the mean for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>Ω</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msub><mi>I</mi><mi>D</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">E[\Omega] =I_D.</annotation></semantics></math>
Next we say we have little certainty about this mean (want high
variance) so we set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
to be close to the lower bound of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
(I often like
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><mi>D</mi><mo>+</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\gamma=D+3</annotation></semantics></math>).
Now we have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>
we need to calculate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
which we do by solving for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
in the equation for the Inverse-Wishart mean<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Note its
&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;D-1&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;
here because
&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mi&gt;Ω&lt;/mi&gt;&lt;annotation encoding="application/x-tex"&gt;\Omega&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;
is
&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;D\times D&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;
rather than
&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;D-1 \times D-1&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/p&gt;'><sup>1</sup></a>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>Ω</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>−</mo><mi>D</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">S = E[\Omega](\gamma -D-1).</annotation></semantics></math>
There you go that’s a prior on the log-absolute-abundances. Next we need
to transform this into a prior on log-ratios. Well the above allows us
to do this by simplifying taking the contrast matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ψ</mi><annotation encoding="application/x-tex">\Psi</annotation></semantics></math>
from the log-ratio transform we want and transforming our prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Σ</mi><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>,</mo><mi>Ψ</mi><mi>S</mi><msup><mi>Ψ</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Sigma \sim W^{-1}(\gamma, \Psi S \Psi^T)</annotation></semantics></math>.
That’s it there the prior on log-ratios built form a prior on
log-absolute-abundances.</p>
<p><em>A Note on Phylogenetic priors:</em> As in phylogenetic linear
models, you can make
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
(as defined above) a covariance derived from the phylogenetic
differences between taxa. This allows you to fit phylogenetic linear
models in <em>fido</em>.</p>
<p><strong>Making It Even Simpler</strong> Say that you have a prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>γ</mi><mo>,</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega \sim W^{-1}(\gamma, S)</annotation></semantics></math>
for covariance between log-absolute-abundances (created as in our
example above). You want to transform this into a prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Σ</mi><mo>∼</mo><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>υ</mi><mo>,</mo><mi>Ξ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Sigma \sim W^{-1}(\upsilon, \Xi)</annotation></semantics></math>.
You do this by simply taking
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>υ</mi><mo>=</mo><mi>γ</mi></mrow><annotation encoding="application/x-tex">\upsilon=\gamma</annotation></semantics></math>.
To calculate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>,
rather than worrying about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ψ</mi><annotation encoding="application/x-tex">\Psi</annotation></semantics></math>,
functions in the <a href="https://jsilve24.github.io/driver/" class="external-link"><em>driver</em> package I
wrote</a> will do this for you, here are recipes:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># To put prior on ALR_j coordinates for some j in (1,...,D-1)</span></span>
<span><span class="va">Xi</span> <span class="op">&lt;-</span> <span class="fu">clrvar2alrvar</span><span class="op">(</span><span class="va">S</span>, <span class="va">j</span><span class="op">)</span></span>
<span><span class="co"># To put prior in a particular ILR coordinate defined by contrast matrix V</span></span>
<span><span class="va">Xi</span> <span class="op">&lt;-</span> <span class="fu">clrvar2ilrvar</span><span class="op">(</span><span class="va">S</span>, <span class="va">V</span><span class="op">)</span></span>
<span><span class="co"># To put prior in CLR coordinates (this one needs two transforms)</span></span>
<span><span class="va">foo</span> <span class="op">&lt;-</span> <span class="fu">clrvar2alrvar</span><span class="op">(</span><span class="va">S</span>, <span class="va">D</span><span class="op">)</span></span>
<span><span class="va">Xi</span> <span class="op">&lt;-</span> <span class="fu">alrvar2clrvar</span><span class="op">(</span><span class="va">foo</span>, <span class="va">D</span><span class="op">)</span></span></code></pre></div>
<p>Hopefully that is simple enough to be useful for folks.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-prior-for-lambda">The prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#the-prior-for-lambda"></a>
</h2>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
are the regression parameters in the linear model. The prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
is just a matrix-normal which we described above:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Λ</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Θ</mi><mo>,</mo><mi>Σ</mi><mo>,</mo><mi>Γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\Lambda \sim N(\Theta, \Sigma, \Gamma).</annotation></semantics></math>
Here
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Θ</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math>
is the mean matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
is actually random (i.e., you don’t have to specify it, its specified by
the prior on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
we discussed already), and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>
is a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mi>x</mi><mi>Q</mi></mrow><annotation encoding="application/x-tex">QxQ</annotation></semantics></math><a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Where
&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;annotation encoding="application/x-tex"&gt;Q&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;
is the number of regression covaraites&lt;/p&gt;'><sup>2</sup></a> covariance
matrix describing the covariance between the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
(<em>i.e.</em>, between the effect of the different covariates). So we
really need to just discuss specifying
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Θ</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math>
and specifying
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>.</p>
<div class="section level3">
<h3 id="choosing-theta">Choosing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Θ</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#choosing-theta"></a>
</h3>
<p>This is really easy, in most situations this will simply be a matrix
of zeros. This implies that you expect that on average, the covariates
of interest are not associated with composition.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;This is the same assumption as used in Ridge
Regression&lt;/p&gt;"><sup>3</sup></a>. This helps prevent
you from inferrign an effect if there isn’t one.</p>
<p>Outside of this simple case lets say you actually have prior
knowledge about the effects of the covariates. Perhaps you have some
knowledge about the mean effect of covariates on log-absolute-abundances
which you describe in a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>×</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">D\times Q</annotation></semantics></math>
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.
Well you can just transform that prior into the log-ratio coordinates
you want as follows:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Transform from log-absolute-abundance effects to effects on absolute-abundances</span></span>
<span><span class="va">foo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">A</span><span class="op">)</span></span>
<span><span class="co"># To put prior on ALR_j coordinates for some j in (1,...,D-1)</span></span>
<span><span class="va">Theta</span> <span class="op">&lt;-</span> <span class="fu">driver</span><span class="fu">::</span><span class="fu">alr_array</span><span class="op">(</span><span class="va">foo</span>, <span class="va">j</span>, parts<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co"># To put prior in a particular ILR coordinate defined by contrast matrix V</span></span>
<span><span class="va">Theta</span> <span class="op">&lt;-</span> <span class="fu">driver</span><span class="fu">::</span><span class="fu">ilr_array</span><span class="op">(</span><span class="va">foo</span>, <span class="va">V</span>, parts<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co"># To put prior in CLR coordinates</span></span>
<span><span class="va">Theta</span> <span class="op">&lt;-</span> <span class="fu">driver</span><span class="fu">::</span><span class="fu">clr_array</span><span class="op">(</span><span class="va">foo</span>, parts<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="choosing-gamma">Choosing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#choosing-gamma"></a>
</h3>
<p>Alright, here we get a break as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>
doesn’t care what log-ratio coordinates your in. It’s just a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>×</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q\times Q</annotation></semantics></math>
covariance matrix describing the covariation between the effects of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
covariates.</p>
<p>For example, Lets say your data is a microbiome survey of a disease
with a number of healthy controls. Your goal is to figure out what is
different between the composition of these two groups. Your model may
have two covariates, an intercept and a binary variable (1 if sample is
from disease, 0 if from healthy). We probably want to set a prior that
allows the intercept to be moderately large but we likely believe that
the differences between disease and health are small (so we want the
effect of the binary covariate to be modest). We could specify:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Γ</mi><mo>=</mo><mi>α</mi><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>.2</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\Gamma = \alpha\begin{bmatrix} 1 &amp; 0 \\ 0&amp; .2 \end{bmatrix}</annotation></semantics></math>
for a scalar
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
which I will discuss in depth below. Note here the off diagonals being
zero also specifies that we don’t think there is any covariation between
the intercept and the effect of the disease state (probably a pretty
good assumption in this example).</p>
<p>The choice of alpha can be important. I will describe it later in the
section on how the choice of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>
interact with the choice of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>.
First I need to briefly describe the prior on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-prior-for-eta">The Prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#the-prior-for-eta"></a>
</h2>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>
are log-ratios from the regression relationship obscured by noise.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>η</mi><mi>j</mi></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><msub><mi>X</mi><mi>j</mi></msub><mo>,</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\eta_j \sim N(\Lambda X_j, \Sigma).</annotation></semantics></math>
Notice
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
shows up again like it did in the prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>.
Actually, there are no more parameters we need to specify, the prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>
is completely induced based on our priors for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>.
The reason I discuss it here is that I want readers to recognize that
the variation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>
about the regression relationship is specified by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>.
That means that if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
is large there is more noise, small there is less noise. This should
also be taken into account when specifying
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>.
The next section will further expand on this idea.</p>
</div>
<div class="section level2">
<h2 id="how-the-choice-of-upsilon-and-xi-interacts-with-the-choice-of-gamma">How the Choice of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>
Interacts With the Choice of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#how-the-choice-of-upsilon-and-xi-interacts-with-the-choice-of-gamma"></a>
</h2>
<p>The point of this subsection is the following, the choice of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
in some senses place a prior on the signal-to-noise ratio in the data.
In short: The larger
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>
is relative to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
(specified by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>)
the more signal, the smaller
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>
is realtive to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
the more noise. I will describe this below.</p>
<p>Notice that we could alternatively write the prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>
as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><mi>X</mi><mo>,</mo><mi>Σ</mi><mo>,</mo><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\eta \sim N(\Lambda X, \Sigma, I)</annotation></semantics></math>
using the matrix normal in parallel to our prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Λ</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Θ</mi><mo>,</mo><mi>Σ</mi><mo>,</mo><mi>Γ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\Lambda \sim N(\Theta, \Sigma, \Gamma).</annotation></semantics></math>
We can write the <em>vec</em> form of these relationships as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mo>⊗</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>Γ</mi><mo>⊗</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align}
vec(\eta) &amp;\sim N(vec(\Lambda X), I\otimes\Sigma) \\
vec(\Lambda) &amp;\sim N(vec(\Theta), \Gamma \otimes \Sigma).
\end{align}
</annotation></semantics></math> If we write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>
as the multiplication of a scalar and scaled matrix (a matrix scaled so
that the sum of the diagonals equals 1)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Γ</mi><mo>=</mo><mi>α</mi><mover><mi>Γ</mi><mo accent="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">\Gamma=\alpha \bar{\Gamma}</annotation></semantics></math>
as we did when describing the choice of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>
above, then the above equations turn into:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>⊗</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Λ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mi>e</mi><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>α</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>Γ</mi><mo accent="true">‾</mo></mover><mo>⊗</mo><mi>Σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align}
vec(\eta) &amp;\sim N(vec(\Lambda X), 1(I\otimes\Sigma)) \\
vec(\Lambda) &amp;\sim N(vec(\Theta), \alpha(\bar{\Gamma}\otimes \Sigma)).
\end{align}
</annotation></semantics></math> and we can see that the magnitude of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
is a factor of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
times the noise level. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&lt;1</annotation></semantics></math>
we have that the the magnitude of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
is smaller than the magnitude of the noise. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha &gt; 1</annotation></semantics></math>
we have that the magnitude of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
is greater than the magnitude of the noise.</p>
<p>The actual “signal” is the product
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Λ</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">\Lambda X</annotation></semantics></math>
(so it depends on the scale of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>)
as well but hopefully the point is clear: <strong>The magnitude of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
(which is specified by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>υ</mi><annotation encoding="application/x-tex">\upsilon</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ξ</mi><annotation encoding="application/x-tex">\Xi</annotation></semantics></math>)
in comparision to the magnitude of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math>
sets the signal-to-noise ratio in our prior.</strong></p>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://www.justin-silverman.com/" class="external-link">Justin Silverman</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
